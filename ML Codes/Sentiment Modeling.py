#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')


# In[2]:


df = pd.read_csv('D:/Downloads/Sabre Hack/Datasets/all_locs.csv')
df.drop('Unnamed: 9', axis=1, inplace=True)
df.head()


# In[3]:


contraction_mapping = {
    "Trump's" : 'trump is',"'cause": 'because',',cause': 'because',';cause': 'because',"ain't": 'am not','ain,t': 'am not',
    'ain;t': 'am not','ainÂ´t': 'am not','ainâ€™t': 'am not',"aren't": 'are not',
    'aren,t': 'are not','aren;t': 'are not','arenÂ´t': 'are not','arenâ€™t': 'are not',"can't": 'cannot',"can't've": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',
    'can;t': 'cannot','can;t;ve': 'cannot have',
    'canÂ´t': 'cannot','canÂ´tÂ´ve': 'cannot have','canâ€™t': 'cannot','canâ€™tâ€™ve': 'cannot have',
    "could've": 'could have','could,ve': 'could have','could;ve': 'could have',"couldn't": 'could not',"couldn't've": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',
    'couldn;t;ve': 'could not have','couldnÂ´t': 'could not',
    'couldnÂ´tÂ´ve': 'could not have','couldnâ€™t': 'could not','couldnâ€™tâ€™ve': 'could not have','couldÂ´ve': 'could have',
    'couldâ€™ve': 'could have',"didn't": 'did not','didn,t': 'did not','didn;t': 'did not','didnÂ´t': 'did not',
    'didnâ€™t': 'did not',"doesn't": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesnÂ´t': 'does not',
    'doesnâ€™t': 'does not',"don't": 'do not','don,t': 'do not','don;t': 'do not','donÂ´t': 'do not','donâ€™t': 'do not',
    "hadn't": 'had not',"hadn't've": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',
    'hadn;t;ve': 'had not have','hadnÂ´t': 'had not','hadnÂ´tÂ´ve': 'had not have','hadnâ€™t': 'had not','hadnâ€™tâ€™ve': 'had not have',"hasn't": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasnÂ´t': 'has not','hasnâ€™t': 'has not',
    "haven't": 'have not','haven,t': 'have not','haven;t': 'have not','havenÂ´t': 'have not','havenâ€™t': 'have not',"he'd": 'he would',
    "he'd've": 'he would have',"he'll": 'he will',
    "he's": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',
    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','heÂ´d': 'he would','heÂ´dÂ´ve': 'he would have','heÂ´ll': 'he will',
    'heÂ´s': 'he is','heâ€™d': 'he would','heâ€™dâ€™ve': 'he would have','heâ€™ll': 'he will','heâ€™s': 'he is',"how'd": 'how did',"how'll": 'how will',
    "how's": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',
    'how;s': 'how is','howÂ´d': 'how did','howÂ´ll': 'how will','howÂ´s': 'how is','howâ€™d': 'how did','howâ€™ll': 'how will',
    'howâ€™s': 'how is',"i'd": 'i would',"i'll": 'i will',"i'm": 'i am',"i've": 'i have','i,d': 'i would','i,ll': 'i will',
    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',"isn't": 'is not',
    'isn,t': 'is not','isn;t': 'is not','isnÂ´t': 'is not','isnâ€™t': 'is not',"it'd": 'it would',"it'll": 'it will',"It's":'it is',
    "it's": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','itÂ´d': 'it would','itÂ´ll': 'it will','itÂ´s': 'it is',
    'itâ€™d': 'it would','itâ€™ll': 'it will','itâ€™s': 'it is',
    'iÂ´d': 'i would','iÂ´ll': 'i will','iÂ´m': 'i am','iÂ´ve': 'i have','iâ€™d': 'i would','iâ€™ll': 'i will','iâ€™m': 'i am',
    'iâ€™ve': 'i have',"let's": 'let us','let,s': 'let us','let;s': 'let us','letÂ´s': 'let us',
    'letâ€™s': 'let us',"ma'am": 'madam','ma,am': 'madam','ma;am': 'madam',"mayn't": 'may not','mayn,t': 'may not','mayn;t': 'may not',
    'maynÂ´t': 'may not','maynâ€™t': 'may not','maÂ´am': 'madam','maâ€™am': 'madam',"might've": 'might have','might,ve': 'might have','might;ve': 'might have',"mightn't": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightnÂ´t': 'might not',
    'mightnâ€™t': 'might not','mightÂ´ve': 'might have','mightâ€™ve': 'might have',"must've": 'must have','must,ve': 'must have','must;ve': 'must have',
    "mustn't": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustnÂ´t': 'must not','mustnâ€™t': 'must not','mustÂ´ve': 'must have',
    'mustâ€™ve': 'must have',"needn't": 'need not','needn,t': 'need not','needn;t': 'need not','neednÂ´t': 'need not','neednâ€™t': 'need not',"oughtn't": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',
    'oughtnÂ´t': 'ought not','oughtnâ€™t': 'ought not',"sha'n't": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',"shan't": 'shall not',
    'shan,t': 'shall not','shan;t': 'shall not','shanÂ´t': 'shall not','shanâ€™t': 'shall not','shaÂ´nÂ´t': 'shall not','shaâ€™nâ€™t': 'shall not',
    "she'd": 'she would',"she'll": 'she will',"she's": 'she is','she,d': 'she would','she,ll': 'she will',
    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','sheÂ´d': 'she would','sheÂ´ll': 'she will',
    'sheÂ´s': 'she is','sheâ€™d': 'she would','sheâ€™ll': 'she will','sheâ€™s': 'she is',"should've": 'should have','should,ve': 'should have','should;ve': 'should have',
    "shouldn't": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldnÂ´t': 'should not','shouldnâ€™t': 'should not','shouldÂ´ve': 'should have',
    'shouldâ€™ve': 'should have',"that'd": 'that would',"that's": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',
    'that;s': 'that is','thatÂ´d': 'that would','thatÂ´s': 'that is','thatâ€™d': 'that would','thatâ€™s': 'that is',"there'd": 'there had',
    "there's": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',
    'thereÂ´d': 'there had','thereÂ´s': 'there is','thereâ€™d': 'there had','thereâ€™s': 'there is',
    "they'd": 'they would',"they'll": 'they will',"they're": 'they are',"they've": 'they have',
    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',
    'they;ve': 'they have','theyÂ´d': 'they would','theyÂ´ll': 'they will','theyÂ´re': 'they are','theyÂ´ve': 'they have','theyâ€™d': 'they would','theyâ€™ll': 'they will',
    'theyâ€™re': 'they are','theyâ€™ve': 'they have',"wasn't": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasnÂ´t': 'was not',
    'wasnâ€™t': 'was not',"we'd": 'we would',"we'll": 'we will',"we're": 'we are',"we've": 'we have','we,d': 'we would','we,ll': 'we will',
    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',
    "weren't": 'were not','weren,t': 'were not','weren;t': 'were not','werenÂ´t': 'were not','werenâ€™t': 'were not','weÂ´d': 'we would','weÂ´ll': 'we will',
    'weÂ´re': 'we are','weÂ´ve': 'we have','weâ€™d': 'we would','weâ€™ll': 'we will','weâ€™re': 'we are','weâ€™ve': 'we have',"what'll": 'what will',"what're": 'what are',"what's": 'what is',
    "what've": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',
    'what;s': 'what is','what;ve': 'what have','whatÂ´ll': 'what will',
    'whatÂ´re': 'what are','whatÂ´s': 'what is','whatÂ´ve': 'what have','whatâ€™ll': 'what will','whatâ€™re': 'what are','whatâ€™s': 'what is',
    'whatâ€™ve': 'what have',"where'd": 'where did',"where's": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',
    'where;s': 'where is','whereÂ´d': 'where did','whereÂ´s': 'where is','whereâ€™d': 'where did','whereâ€™s': 'where is',
    "who'll": 'who will',"who's": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',
    'whoÂ´ll': 'who will','whoÂ´s': 'who is','whoâ€™ll': 'who will','whoâ€™s': 'who is',"won't": 'will not','won,t': 'will not','won;t': 'will not',
    'wonÂ´t': 'will not','wonâ€™t': 'will not',"wouldn't": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldnÂ´t': 'would not',
    'wouldnâ€™t': 'would not',"you'd": 'you would',"you'll": 'you will',"you're": 'you are','you,d': 'you would','you,ll': 'you will',
    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',
    'you;re': 'you are','youÂ´d': 'you would','youÂ´ll': 'you will','youÂ´re': 'you are','youâ€™d': 'you would','youâ€™ll': 'you will','youâ€™re': 'you are',
    'Â´cause': 'because','â€™cause': 'because',"you've": "you have","could'nt": 'could not',
    "havn't": 'have not',"hereâ€™s": "here is",'i""m': 'i am',"i'am": 'i am',"i'l": "i will","i'v": 'i have',"wan't": 'want',"was'nt": "was not","who'd": "who would",
    "who're": "who are","who've": "who have","why'd": "why would","would've": "would have","y'all": "you all","y'know": "you know","you.i": "you i",
    "your'e": "you are","arn't": "are not","agains't": "against","c'mon": "common","doens't": "does not",'don""t': "do not","dosen't": "does not",
    "dosn't": "does not","shoudn't": "should not","that'll": "that will","there'll": "there will","there're": "there are",
    "this'll": "this all","u're": "you are", "ya'll": "you all","you'r": "you are","youâ€™ve": "you have","d'int": "did not","did'nt": "did not","din't": "did not","dont't": "do not","gov't": "government",
    "i'ma": "i am","is'nt": "is not","â€˜I":'I',
    'á´€É´á´…':'and','á´›Êœá´‡':'the','Êœá´á´á´‡':'home','á´œá´˜':'up','Ê™Ê':'by','á´€á´›':'at','â€¦and':'and','civilbeat':'civil beat',\
    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','á´„Êœá´‡á´„á´‹':'check','Ò“á´Ê€':'for','á´›ÊœÉªs':'this','á´„á´á´á´˜á´œá´›á´‡Ê€':'computer',\
    'á´á´É´á´›Êœ':'month','á´¡á´Ê€á´‹ÉªÉ´É¢':'working','á´Šá´Ê™':'job','Ò“Ê€á´á´':'from','Sá´›á´€Ê€á´›':'start','gubmit':'submit','COâ‚‚':'carbon dioxide','Ò“ÉªÊ€sá´›':'first',\
    'á´‡É´á´…':'end','á´„á´€É´':'can','Êœá´€á´ á´‡':'have','á´›á´':'to','ÊŸÉªÉ´á´‹':'link','á´Ò“':'of','Êœá´á´œÊ€ÊŸÊ':'hourly','á´¡á´‡á´‡á´‹':'week','á´‡É´á´…':'end','á´‡xá´›Ê€á´€':'extra',\
    'GÊ€á´‡á´€á´›':'great','sá´›á´œá´…á´‡É´á´›s':'student','sá´›á´€Ê':'stay','á´á´á´s':'mother','á´Ê€':'or','á´€É´Êá´É´á´‡':'anyone','É´á´‡á´‡á´…ÉªÉ´É¢':'needing','á´€É´':'an','ÉªÉ´á´„á´á´á´‡':'income',\
    'Ê€á´‡ÊŸÉªá´€Ê™ÊŸá´‡':'reliable','Ò“ÉªÊ€sá´›':'first','Êá´á´œÊ€':'your','sÉªÉ¢É´ÉªÉ´É¢':'signing','Ê™á´á´›á´›á´á´':'bottom','Ò“á´ÊŸÊŸá´á´¡ÉªÉ´É¢':'following','Má´€á´‹á´‡':'make',\
    'á´„á´É´É´á´‡á´„á´›Éªá´É´':'connection','ÉªÉ´á´›á´‡Ê€É´á´‡á´›':'internet','financialpost':'financial post', 'Êœaá´ á´‡':' have ', 'á´„aÉ´':' can ', 'Maá´‹á´‡':' make ', 'Ê€á´‡ÊŸÉªaÊ™ÊŸá´‡':' reliable ', 'É´á´‡á´‡á´…':' need ',
    'á´É´ÊŸÊ':' only ', 'á´‡xá´›Ê€a':' extra ', 'aÉ´':' an ', 'aÉ´Êá´É´á´‡':' anyone ', 'sá´›aÊ':' stay ', 'Sá´›aÊ€á´›':' start', 'SHOPO':'shop',
    }


# In[4]:


def clean_contractions(text, mapping):
    specials = ["â€™", "â€˜", "Â´", "`"]
    for s in specials:
        text = text.replace(s, "'")
    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(" ")])
    return text


# In[5]:


df['Description_model'] = df['Description'].apply(lambda x: clean_contractions(x, contraction_mapping))


# In[6]:


punct = "/-'?!.,#$%\'()*+-/:;<=>@[\\]^_`{|}~" + '""â€œâ€â€™' + 'âˆÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\Ã—â„¢âˆšÂ²â€”â€“&'

def unknown_punct(embed, punct):
    unknown = ''
    for p in punct:
        if p not in embed:
            unknown += p
            unknown += ' '
    return unknown


# In[7]:


punct_mapping = {"â€˜": "'", "â‚¹": "e", "Â´": "'", "Â°": "", "â‚¬": "e", "â„¢": "tm", "âˆš": " sqrt ", "Ã—": "x", "Â²":                  "2", "â€”": "-", "â€“": "-", "â€™": "'", "_": "-", "`": "'", 'â€œ': '"', 'â€': '"', 'â€œ': '"', "Â£": "e", 'âˆ':                 'infinity', 'Î¸': 'theta', 'Ã·': '/', 'Î±': 'alpha', 'â€¢': '.', 'Ã ': 'a', 'âˆ’': '-', 'Î²': 'beta', 'âˆ…': '', 'Â³': '3',                 'Ï€': 'pi', }

def clean_special_chars(text, punct, mapping):
    for p in mapping:
        text = text.replace(p, mapping[p])
    for p in punct:
        text = text.replace(p, f' {p} ')
    specials = {'\u200b': ' ', 'â€¦': ' ... ', '\ufeff': '', 'à¤•à¤°à¤¨à¤¾': '', 'à¤¹à¥ˆ': ''}  # Other special characters that I have to deal with in last
    for s in specials:
        text = text.replace(s, specials[s])
    return text

df['Description_model'] = df['Description_model'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))


# In[8]:


mispell_dict = {'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','GÊ€á´‡at':'great','Ê™á´á´›toá´':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yá´á´œ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','á´€':'a', 'ğŸ˜‰':'wink','ğŸ˜‚':'joy','ğŸ˜€':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}

def correct_spelling(x, dic):
    for word in dic.keys():
        x = x.replace(word, dic[word])
    return x

df['Description_model'] = df['Description_model'].apply(lambda x: correct_spelling(x, mispell_dict))


# In[9]:


import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

sia= SentimentIntensityAnalyzer()


# In[10]:


df['Place_sentiment'] = df['Description_model'].apply(lambda x: sia.polarity_scores(x)['compound'])
df['Place_sentiment'] = df['Place_sentiment'].round(2)


# In[11]:


df.dtypes


# In[12]:


df_sent = pd.DataFrame(df.groupby(['State'])['Place_sentiment'].mean())
df_sent = df_sent.rename(columns={'Place_sentiment':'Overall_state_sentiment'},).reset_index()
df_sent['Overall_state_sentiment'] = df_sent['Overall_state_sentiment'].round(2)


# In[13]:


df_sent.head(2)


# In[14]:


df_all = pd.merge(df,df_sent, on=['State'], how='left')


# In[16]:


df_all.head(2)


# In[ ]:




